{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b58db9",
   "metadata": {},
   "source": [
    "\"INSTALLING DEPENDENCIES\"\n",
    "# %pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfbc21db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\OneDrive\\Desktop\\mlpy\\multimodal_AI\\MMAIenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"IMPORTING LIBRARIES\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gradio as gr\n",
    "from accelerate import Accelerator\n",
    "from pympler import asizeof as size\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210af2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device used is cpu\n",
      "Downloading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:  50%|█████     | 1/2 [00:00<00:00,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor already exists, skipping download.\n",
      "Model already exists, skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2/2 [00:00<00:00,  9.39it/s]\n",
      "Downloading:  50%|█████     | 1/2 [00:00<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 tokenizer already saved, skipping download.\n",
      " gpt2 Model already saved, skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2/2 [00:00<00:00,  9.16it/s]\n",
      "Downloading: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable diffuse already saved, skipping download.\n"
     ]
    }
   ],
   "source": [
    "\"INITIALIZING THE DATASET\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'device used is {device}')\n",
    "try:\n",
    "    os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "    print(\"Downloading models...\")\n",
    "    for step in tqdm([\"processor\", \"model\"], desc=\"Downloading\"):\n",
    "        if step == \"processor\":\n",
    "            if os.path.exists('./clip_processor'):\n",
    "                print(\"Processor already exists, skipping download.\")\n",
    "                pass\n",
    "            else:\n",
    "                clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "                clip_processor.save_pretrained('./clip_processor')\n",
    "        else:\n",
    "            if os.path.exists('./clip_model'):\n",
    "                print(\"Model already exists, skipping download.\")\n",
    "                pass\n",
    "            else:\n",
    "                clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "                clip_model.save_pretrained('./clip_model')                \n",
    "        time.sleep(0.1)\n",
    "\n",
    "    for step in tqdm([\"tokenizer\", \"model\"], desc=\"Downloading\"):\n",
    "        if step == \"tokenizer\":\n",
    "            if os.path.exists('./gpt2_tokenizer'):\n",
    "                print(\"gpt2 tokenizer already saved, skipping download.\")\n",
    "                pass\n",
    "            else:\n",
    "                gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "                gpt2_tokenizer.save_pretrained('./gpt2_tokenizer')\n",
    "        else:\n",
    "            if os.path.exists('./gpt2_model'):\n",
    "                print(\" gpt2 Model already saved, skipping download.\")\n",
    "                pass\n",
    "            else:\n",
    "                gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "                gpt2_model.save_pretrained('./gpt2_model')\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    for step in tqdm([\"diffuse\"], desc=\"Downloading\"):\n",
    "        if step == \"diffuse\":\n",
    "            if os.path.exists('./stable_diffuse'):\n",
    "                print(\"stable diffuse already saved, skipping download.\")\n",
    "                pass\n",
    "            else:\n",
    "                stable_diffuse = StableDiffusionPipeline.from_pretrained(\n",
    "                    \"runwayml/stable-diffusion-v1-5\",\n",
    "                    torch_dtype=torch.float16\n",
    "                ).to(device)\n",
    "                stable_diffuse.save_pretrained('./stable_diffuse')\n",
    "        time.sleep(0.1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please check your internet connection or the model names.\")\n",
    "BASE_DIR = './data/'\n",
    "sub_dirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))] \n",
    "TEXT_PATH = [os.path.join(BASE_DIR, sub_dir, 'text.json')\n",
    "             for sub_dir in sub_dirs if os.path.isdir(os.path.join(BASE_DIR, sub_dir))]\n",
    "IMG_PATH = [os.path.join(BASE_DIR, sub_dir, 'img', 'meta.json')\n",
    "            for sub_dir in sub_dirs if os.path.isdir(os.path.join(BASE_DIR, sub_dir))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb75f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "'READING THE DATA AND CREATING THE MMD'\n",
    "def mm_data(txt_paths=TEXT_PATH, img_paths=IMG_PATH):\n",
    "    \"\"\"Create a multimodal dataset (MMD) from text and image files.\"\"\"\n",
    "    for i, (txt_path, img_path) in enumerate(zip(txt_paths, img_paths)):\n",
    "        print(f\"n--- Processing pair {i+1} ---\")\n",
    "        print(f\"Text file: {txt_path}\")\n",
    "        print(f\"Image file: {img_path}\")\n",
    "        \n",
    "        if not txt_path.endswith(\".json\") or not img_path.endswith(\".json\"):\n",
    "            continue\n",
    "         \n",
    "        combined_entries = {}\n",
    "        \n",
    "        # Process text file\n",
    "        if txt_path:\n",
    "            try:\n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    text_data = json.load(f)\n",
    "                    print(f\"✓ Text data loaded: {len(text_data) if hasattr(text_data, '__len__') else 'N/A'} items\")\n",
    "                    combined_entries['text_data'] = text_data\n",
    "                    print(f'size of text data: {size.asizeof(combined_entries['text_data'])/ (1024):.2f}kb')\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error reading text file: {e}\")\n",
    "\n",
    "        # Process image file\n",
    "        if img_path:\n",
    "            try:\n",
    "                with open(img_path, 'r', encoding='utf-8') as f:\n",
    "                    img_data = json.load(f)\n",
    "                    print(f\"✓ Image data loaded: {len(img_data) if hasattr(img_data, '__len__') else 'N/A'} items\")\n",
    "                    combined_entries['image_data'] = img_data\n",
    "                    print(f'size of image data: {size.asizeof(combined_entries['image_data'])/ (1024):.2f}kb')\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error reading image file: {e}\")\n",
    "\n",
    "        if combined_entries:\n",
    "            yield combined_entries\n",
    "        else:\n",
    "            print(f\"✗ No data found for pair {i+1}, skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245cfe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'CLEANING THE DATA'\n",
    "def clean_data(data):\n",
    "    \"\"\"Comprehensive text cleaning function\"\"\"\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    data = re.sub(r'<[^>]+>', '', data)\n",
    "    \n",
    "    # Remove URLs\n",
    "    data = re.sub(r'httpS+|wwwS+|httpsS+', '', data, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove excessive punctuation but keep basic ones\n",
    "    data = re.sub(r'[^a-zA-Z0-9s.,;:!?-]', '', data)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    data = re.sub(r'S+@S+', '', data)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    data = data.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    data = re.sub(r's+', ' ', data).strip()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfac80e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_feature_dim(df):\n",
    "    \"\"\"Infers the dimension of the image features from the first valid entry.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Warning: DataFrame is empty. using default of 512.\")\n",
    "        return 512\n",
    "    for _, entry in df.iterrows():\n",
    "        if 'image_data' in entry and pd.notna(entry['image_data']):\n",
    "            image_data = entry['image_data']\n",
    "            if isinstance(image_data, dict) and 'img_meta' in image_data:\n",
    "                for img_item in image_data['img_meta']:\n",
    "                    features = img_item.get('features', [])\n",
    "                    if features and len(features) > 0:\n",
    "                        print(f'feature_dim generated, length is {len(features)}')\n",
    "                        # Return the length of the first valid feature list\n",
    "                        return len(features)\n",
    "    raise ValueError(\"Could not infer feature dimension using default of 512.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d543c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'DATA TOKENIZATION AND EMMBEDDING USING TRANSFORMERS'\n",
    "class TFMultimodalProcessor:\n",
    "    \"\"\"A simple multimodal processor that extracts text and image features.\"\"\"\n",
    "    def __init__(self, result_df, feature_dim):\n",
    "        self.df = result_df\n",
    "        if os.path.isdir(\"./clip_processor\") and os.path.isdir(\"./clip_model\"):\n",
    "            self.clip_processor = CLIPProcessor.from_pretrained(\"./clip_processor\", local_files_only=True)\n",
    "            self.clip_model = CLIPModel.from_pretrained(\"./clip_model\", local_files_only=True)\n",
    "        else:\n",
    "            self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.feature_dim = feature_dim\n",
    "        self.text_embedding_dim = self.clip_model.config.text_config.hidden_size\n",
    "    \n",
    "    def extract_text(self, entry):\n",
    "        \"\"\"Extract all text from both text_data and image_data\"\"\"\n",
    "        all_text = []\n",
    "        \n",
    "        # From text_data\n",
    "        if 'text_data' in entry and pd.notna(entry['text_data']):\n",
    "            text_data = entry['text_data']\n",
    "            if isinstance(text_data, str):\n",
    "                cleaned_text = clean_data(text_data)\n",
    "                all_text.append(cleaned_text)\n",
    "            elif isinstance(text_data, dict):\n",
    "                # Extract common text fields\n",
    "                for key in ['title', 'content', 'description', 'text', 'summary', 'wikitext']:\n",
    "                    if key in text_data and text_data[key]:\n",
    "                        cleaned_text = clean_data(str(text_data[key]))[:1500]\n",
    "                        all_text.append(cleaned_text)\n",
    "        \n",
    "        # From image_data (descriptions, captions, etc.)\n",
    "        if 'image_data' in entry and pd.notna(entry['image_data']):\n",
    "            image_data = entry['image_data']\n",
    "            if isinstance(image_data, dict) and 'img_meta' in image_data:\n",
    "                for img_item in image_data['img_meta']:\n",
    "                    for key in ['description', 'caption', 'title', 'parsed_title']:\n",
    "                        if key in img_item and img_item[key]:\n",
    "                            value = img_item[key]\n",
    "                            if isinstance(value, list):\n",
    "                                cleaned_text = [clean_data(str(v)) for v in value if v]\n",
    "                                all_text.extend(cleaned_text)\n",
    "                            else:\n",
    "                                cleaned_text = clean_data(str(value))\n",
    "                                all_text.append(cleaned_text)\n",
    "        \n",
    "        return \" \".join(all_text)\n",
    "    \n",
    "    def process_and_embed(self, df, BATCH_SIZE=64):\n",
    "        \"\"\"Process all entry to create unified embedding\"\"\"        \n",
    "        for i in range(0, len(df), BATCH_SIZE):\n",
    "            # GET BATCH DATA\n",
    "            print(f\"--- Processing batch {i // BATCH_SIZE + 1} ---\")\n",
    "            batch_df = df.iloc[i: i + BATCH_SIZE]\n",
    "\n",
    "            extracted_text = [self.extract_text(entry) for _, entry in batch_df.iterrows()]\n",
    "            text_embedding = None\n",
    "            if extracted_text:\n",
    "                inputs = self.clip_processor(text=extracted_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                with torch.no_grad():\n",
    "                    text_embedding = self.clip_model.get_text_features(**inputs).cpu()\n",
    "\n",
    "            # Get image features if available\n",
    "            image_features = []\n",
    "            for _, entry in batch_df.iterrows():\n",
    "                image_data = entry.get('image_data', {})\n",
    "                features = []\n",
    "                if isinstance(image_data, dict) and 'img_meta' in image_data: \n",
    "                    for img_item in image_data['img_meta']:\n",
    "                        features.extend(img_item.get('features', []))\n",
    "\n",
    "                # tokenize the features\n",
    "                if features:\n",
    "                    float_features = [float(f) for f in features]\n",
    "                    image_features.append(torch.tensor(float_features, dtype=torch.float32))\n",
    "                else:\n",
    "                    image_features.append(torch.zeros(self.feature_dim, dtype=torch.float32))\n",
    "\n",
    "            if not image_features:\n",
    "                image_features = [torch.zeros(self.feature_dim, dtype=torch.float32) for _ in range(len(batch_df))]  \n",
    "            max_dim = max([f.shape[0] for f in image_features]) if image_features else self.feature_dim\n",
    "            max_dim = max(max_dim, self.feature_dim)\n",
    "            # Pad all image features to the max dimension of the batch\n",
    "            padded_image_features = []\n",
    "            for f in image_features:\n",
    "                if f.shape[0] < max_dim:\n",
    "                    padded = torch.cat([f, torch.zeros(max_dim - f.shape[0], dtype=torch.float32)])\n",
    "                else:\n",
    "                    padded = f[:max_dim]\n",
    "                padded_image_features.append(padded)\n",
    "            image_embedding = torch.stack(padded_image_features).cpu()\n",
    "\n",
    "            # 3. Concatenate and yield the combined embedding\n",
    "            # The embeddings are concatenated as-is. This is the correct multimodal approach.\n",
    "            # The `MultimodalBot` we designed will have an `input_projection` layer that learns\n",
    "            # how to handle this combined, heterogeneous embedding.\n",
    "            combined_embeddings = torch.cat((text_embedding, image_embedding), dim=1).numpy()\n",
    "\n",
    "            # Ensure shapes match before yielding\n",
    "            if combined_embeddings.shape[0] == len(batch_df):\n",
    "                yield batch_df, combined_embeddings\n",
    "            else:\n",
    "                print(\"Warning: Skipping this batch due to mismatched embedding and dataframe sizes.\")\n",
    "\n",
    "            del text_embedding, image_embedding, combined_embeddings\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2543db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'MULTIMODAL GENERATION PIPELINE'\n",
    "class MultimodalGenerator:\n",
    "    def __init__(self, bot_model, processor, device=device):\n",
    "        # The bot_model is the new, trainable multimodal model\n",
    "        self.bot_model = bot_model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.accelerator = Accelerator()\n",
    "        print(f\"Using device for clip: {self.accelerator.device}\")\n",
    "        \n",
    "        # We get the LLM tokenizer from the bot_model, which is our new bot.\n",
    "        self.llm_tokenizer = self.bot_model.tokenizer\n",
    "\n",
    "        # Load Stable Diffusion\n",
    "        if os.path.isdir(\"./stable_diffuse\"):\n",
    "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                \"./stable_diffuse\", local_files_only=True, torch_dtype=torch.float16,\n",
    "                device_map=\"balanced\"\n",
    "            )\n",
    "        else:\n",
    "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                \"runwayml/stable-diffusion-v1-5\",\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"balanced\"\n",
    "            )\n",
    "        self.pipe = self.accelerator.prepare(self.pipe)\n",
    "\n",
    "        # Load CLIP for evaluation from the processor\n",
    "        self.clip_processor = self.processor.clip_processor\n",
    "        self.clip_model = self.processor.clip_model\n",
    "\n",
    "        # Create output folder\n",
    "        os.makedirs(\"generated_outputs\", exist_ok=True)\n",
    "\n",
    "    def create_combined_embedding(self, user_text, user_image=None):\n",
    "        # 1. Get text embedding\n",
    "        text_inputs = self.clip_processor(text=[user_text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            text_embedding = self.clip_model.get_text_features(**text_inputs).cpu()\n",
    "        \n",
    "        # 2. Get image embedding\n",
    "        if user_image:\n",
    "            image_inputs = self.clip_processor(images=user_image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                image_embedding = self.clip_model.get_image_features(**image_inputs).cpu()\n",
    "        else:\n",
    "            # If no image is provided, use a zero vector of the correct size\n",
    "            clip_image_dim = self.clip_model.config.vision_config.hidden_size\n",
    "            image_embedding = torch.zeros(1, clip_image_dim)\n",
    "        \n",
    "        # 3. Concatenate the embeddings\n",
    "        combined_embedding = torch.cat((text_embedding, image_embedding), dim=1)\n",
    "        return combined_embedding.to(self.device)\n",
    "    \n",
    "    def refine_prompt_multimodal(self, combined_embedding):\n",
    "        with torch.no_grad():\n",
    "            refined_prompts = self.bot_model(combined_embedding.to(self.accelerator.device))\n",
    "        return refined_prompts[0]\n",
    "\n",
    "    def generate_image(self, prompt):\n",
    "        image = self.pipe(prompt, guidance_scale=7.5).images[0]\n",
    "        return image\n",
    "\n",
    "    def evaluate_clip_similarity(self, image, text):\n",
    "        inputs = self.clip_processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(self.accelerator.device)\n",
    "        outputs = self.clip_model(**inputs)\n",
    "        similarity = torch.cosine_similarity(outputs.image_embeds, outputs.text_embeds).item()\n",
    "        return similarity\n",
    "\n",
    "    def run(self, data, limit=5):\n",
    "        processed_batches = self.processor.process_and_embed(data)\n",
    "        limit_count = 0\n",
    "        \n",
    "        # Loop through the generator to get batch-level data and embeddings\n",
    "        for batch_df, batch_embeddings in processed_batches:\n",
    "            if limit_count >= limit:\n",
    "                print(\"Limit reached, stopping.\")\n",
    "                return\n",
    "\n",
    "            # Feed the embeddings into our new bot to get prompts\n",
    "            refined_prompts = self.bot_model(torch.tensor(batch_embeddings).float().unsqueeze(1))\n",
    "            \n",
    "            # Now, iterate through the individual entries and use the generated prompts\n",
    "            batch_index = 0\n",
    "            for i, row in batch_df.iterrows():\n",
    "                if limit_count >= limit:\n",
    "                    break\n",
    "\n",
    "                raw_text = self.processor.extract_text(row)\n",
    "                if not raw_text.strip():\n",
    "                    print(f\"⚠️ Skipping empty text at index {limit_count}\")\n",
    "                    limit_count += 1\n",
    "                    continue\n",
    "\n",
    "                refined_prompt = refined_prompts[batch_index % len(refined_prompts)] # Get the correct prompt for the row\n",
    "                image = self.generate_image(refined_prompt)\n",
    "                similarity = self.evaluate_clip_similarity(image, raw_text)\n",
    "\n",
    "                # Save image and log\n",
    "                image_path = f\"generated_outputs/image_{limit_count+1}.png\"\n",
    "                image.save(image_path)\n",
    "\n",
    "                print(f\"n📌 Entry {limit_count+1}\")\n",
    "                print(f\"🔤 Original Text: {raw_text[:100]}...\")\n",
    "                print(f\"🧠 Refined Prompt: {refined_prompt}\")\n",
    "                print(f\"🎯 CLIP Similarity: {similarity:.4f}\")\n",
    "                print(f\"🖼️ Saved to: {image_path}\")\n",
    "                \n",
    "                limit_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84070e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'TRAINING THE MODEL'\n",
    "class MultimodalBotSoftPrompt(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal bot that uses the combined embedding as a soft prompt.\n",
    "    The combined embedding is prepended to the input token embeddings,\n",
    "    allowing the LLM's attention mechanism to process it directly.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained LLM and tokenizer\n",
    "        if os.path.isdir(\"./gpt2_tokenizer\") and os.path.isdir(\"./gpt2_model\"):\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_tokenizer\", local_files_only=True)\n",
    "            self.llm = GPT2LMHeadModel.from_pretrained(\"./gpt2_model\", local_files_only=True)\n",
    "        else:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "            self.llm = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        \n",
    "        # We need to project the combined embedding to the same dimension as the\n",
    "        # LLM's token embeddings (GPT-2's hidden size is 768)\n",
    "        self.input_projection = nn.Linear(embedding_dim, self.llm.config.hidden_size)\n",
    "\n",
    "        # Set the tokenizer's padding token\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.llm.config.pad_token_id = self.llm.config.eos_token_id\n",
    "        \n",
    "        # Freeze the main LLM weights to focus training on the projection layer\n",
    "        for param in self.llm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # The projection layer is trainable\n",
    "        self.input_projection.requires_grad = True\n",
    "\n",
    "    def forward(self, combined_embedding):\n",
    "        # Check if the input is 2D and unsqueeze it to make it 3D\n",
    "        # Expected shape: (batch_size, 1, embedding_dim)\n",
    "        if combined_embedding.dim() == 2:\n",
    "            combined_embedding = combined_embedding.unsqueeze(1)\n",
    "        # 1. Project the combined embedding to the LLM's embedding space\n",
    "        # Shape: (batch_size, embedding_dim) -> (batch_size, 1, hidden_size)\n",
    "        projected_embedding = self.input_projection(combined_embedding)\n",
    "        \n",
    "        # 2. Start with a simple \"starter\" prompt. This is not strictly necessary but\n",
    "        # can help guide the generation.\n",
    "        starter_prompt_text = \"Generate a vivid prompt for an image based on the input features:\"\n",
    "        \n",
    "        # 3. Tokenize the text prompt\n",
    "        input_tokens = self.tokenizer(starter_prompt_text, return_tensors=\"pt\")\n",
    "        \n",
    "        # 4. Get the original token embeddings from the LLM\n",
    "        # This requires manually accessing the LLM's embedding layer\n",
    "        text_embeddings = self.llm.get_input_embeddings()(input_tokens.input_ids.to(projected_embedding.device))\n",
    "        \n",
    "        # 5. Concatenate the projected embedding with the text embeddings\n",
    "        # This is the \"soft prompt\" fusion!\n",
    "        # The multimodal information is now at the very beginning of the sequence.\n",
    "        batch_size = projected_embedding.size(0)\n",
    "        if batch_size > 1:\n",
    "            text_embeddings_expanded = text_embeddings.expand(batch_size, -1, -1)\n",
    "        else:\n",
    "            text_embeddings_expanded = text_embeddings\n",
    "        fused_input_embeddings = torch.cat([projected_embedding, text_embeddings_expanded], dim=1)\n",
    "\n",
    "        # 6. Create an attention mask for the fused input\n",
    "        # The mask should be all 1s since all tokens are relevant\n",
    "        # The length is the combined length of the soft prompt and the starter prompt\n",
    "        attention_mask = torch.ones(\n",
    "        projected_embedding.size(0), \n",
    "        fused_input_embeddings.size(1), \n",
    "        dtype=torch.long,\n",
    "        device=projected_embedding.device\n",
    "        )\n",
    "\n",
    "        # 7. Generate text using the fused embeddings\n",
    "        # The `inputs_embeds` parameter allows us to bypass the standard token lookup\n",
    "        output = self.llm.generate(\n",
    "            inputs_embeds=fused_input_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=self.llm.config.pad_token_id,\n",
    "            eos_token_id=self.llm.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode and return the generated prompts\n",
    "        refined_prompts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in output]\n",
    "        return refined_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded46b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING DATASET WITH SMART SAMPLING ===\n",
      "Creating smart limited dataset: 3 files, 50 items each\n",
      "Sampling strategy: diverse\n",
      "n--- Processing pair 1 ---\n",
      "Text file: ./data/...And_Justice_for_All_(album)\\text.json\n",
      "Image file: ./data/...And_Justice_for_All_(album)\\img\\meta.json\n",
      "✓ Text data loaded: 5 items\n",
      "size of text data: 667.54kb\n",
      "✓ Image data loaded: 1 items\n",
      "size of image data: 130.54kb\n",
      "  → Images: kept all 1 features\n",
      "✓ Added file 1/3\n",
      "n--- Processing pair 2 ---\n",
      "Text file: ./data/0.999\\text.json\n",
      "Image file: ./data/0.999\\img\\meta.json\n",
      "✓ Text data loaded: 420314 items\n",
      "size of text data: 410.51kb\n",
      "✓ Image data loaded: 169008 items\n",
      "size of image data: 165.09kb\n",
      "✓ Added file 2/3\n",
      "n--- Processing pair 3 ---\n",
      "Text file: ./data/1080┬░_Snowboarding\\text.json\n",
      "Image file: ./data/1080┬░_Snowboarding\\img\\meta.json\n",
      "✓ Text data loaded: 145973 items\n",
      "size of text data: 142.59kb\n",
      "✓ Image data loaded: 16 items\n",
      "size of image data: 0.06kb\n",
      "✓ Added file 3/3\n",
      "n--- Processing pair 4 ---\n",
      "Text file: ./data/1257_Samalas_eruption\\text.json\n",
      "Image file: ./data/1257_Samalas_eruption\\img\\meta.json\n",
      "✓ Text data loaded: 506178 items\n",
      "size of text data: 494.36kb\n",
      "✓ Image data loaded: 78194 items\n",
      "size of image data: 76.41kb\n",
      "Reached limit of 3 files, stopping...\n",
      "\n",
      "=== SMART DATASET CREATED ===\n",
      "DataFrame shape: (3, 2)\n",
      "Memory usage: 0.70 MB\n",
      "STEP 1: Creating development dataset for initial testing...\n",
      "\n",
      "=== CREATING DEVELOPMENT DATASET ===\n",
      "Target: 3 files × 50 items = ~150 samples\n",
      "Creating smart limited dataset: 3 files, 50 items each\n",
      "Sampling strategy: diverse\n",
      "n--- Processing pair 1 ---\n",
      "Text file: ./data/...And_Justice_for_All_(album)\\text.json\n",
      "Image file: ./data/...And_Justice_for_All_(album)\\img\\meta.json\n",
      "✓ Text data loaded: 5 items\n",
      "size of text data: 667.54kb\n",
      "✓ Image data loaded: 1 items\n",
      "size of image data: 130.54kb\n",
      "  → Images: kept all 1 features\n",
      "✓ Added file 1/3\n",
      "n--- Processing pair 2 ---\n",
      "Text file: ./data/0.999\\text.json\n",
      "Image file: ./data/0.999\\img\\meta.json\n",
      "✓ Text data loaded: 420314 items\n",
      "size of text data: 410.51kb\n",
      "✓ Image data loaded: 169008 items\n",
      "size of image data: 165.09kb\n",
      "✓ Added file 2/3\n",
      "n--- Processing pair 3 ---\n",
      "Text file: ./data/1080┬░_Snowboarding\\text.json\n",
      "Image file: ./data/1080┬░_Snowboarding\\img\\meta.json\n",
      "✓ Text data loaded: 145973 items\n",
      "size of text data: 142.59kb\n",
      "✓ Image data loaded: 16 items\n",
      "size of image data: 0.06kb\n",
      "✓ Added file 3/3\n",
      "n--- Processing pair 4 ---\n",
      "Text file: ./data/1257_Samalas_eruption\\text.json\n",
      "Image file: ./data/1257_Samalas_eruption\\img\\meta.json\n",
      "✓ Text data loaded: 506178 items\n",
      "size of text data: 494.36kb\n",
      "✓ Image data loaded: 78194 items\n",
      "size of image data: 76.41kb\n",
      "Reached limit of 3 files, stopping...\n",
      "\n",
      "✅ SUCCESS: development dataset created\n",
      "Shape: (3, 2)\n",
      "Memory: 0.70 MB\n",
      "\n",
      "📊 CURRENT DATASET INFO:\n",
      "Purpose: Development (for initial testing)\n",
      "Samples: ~150 (estimated)\n",
      "\n",
      "🚀 TO SCALE UP FOR TRAINING:\n",
      "   combined_dataframe = upgrade_dataset(combined_dataframe, 'training_small')\n",
      "   # This will give you ~5,000 samples for basic training\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# MEMORY-EFFICIENT SOLUTION: Create sample dataset instead of loading all data\n",
    "# IMPROVED DATASET CREATION WITH SMART SAMPLING\n",
    "# Add this function to replace your current create_limited_dataframe\n",
    "\n",
    "def create_smart_limited_dataframe(max_files=5, max_items_per_file=100, sampling_strategy='diverse'):\n",
    "    \"\"\"Create limited DataFrame with smart sampling to preserve feature diversity\"\"\"\n",
    "    import random\n",
    "    sample_data = []\n",
    "    file_count = 0\n",
    "    \n",
    "    print(f\"Creating smart limited dataset: {max_files} files, {max_items_per_file} items each\")\n",
    "    print(f\"Sampling strategy: {sampling_strategy}\")\n",
    "    \n",
    "    for data_entry in mm_data(TEXT_PATH, IMG_PATH):\n",
    "        if file_count >= max_files:\n",
    "            print(f\"Reached limit of {max_files} files, stopping...\")\n",
    "            break\n",
    "            \n",
    "        limited_entry = {}\n",
    "        \n",
    "        # Handle text data\n",
    "        if 'text_data' in data_entry:\n",
    "            text_data = data_entry['text_data']\n",
    "            if isinstance(text_data, list) and len(text_data) > max_items_per_file:\n",
    "                if sampling_strategy == 'diverse':\n",
    "                    # Sample evenly across the dataset\n",
    "                    step = len(text_data) // max_items_per_file\n",
    "                    sampled_text = [text_data[i] for i in range(0, len(text_data), step)][:max_items_per_file]\n",
    "                    limited_entry['text_data'] = sampled_text\n",
    "                    print(f\"  → Text: sampled {len(sampled_text)} from {len(text_data)} (diverse)\")\n",
    "                else:\n",
    "                    limited_entry['text_data'] = text_data[:max_items_per_file]\n",
    "            else:\n",
    "                limited_entry['text_data'] = text_data\n",
    "        \n",
    "        # Handle image data with SMART SAMPLING - KEY IMPROVEMENT!\n",
    "        if 'image_data' in data_entry:\n",
    "            img_data = data_entry['image_data']\n",
    "            if isinstance(img_data, dict) and 'img_meta' in img_data:\n",
    "                img_meta = img_data['img_meta']\n",
    "                \n",
    "                if len(img_meta) > max_items_per_file:\n",
    "                    if sampling_strategy == 'diverse':\n",
    "                        # DIVERSE SAMPLING: Get features from across the entire dataset\n",
    "                        step = len(img_meta) // max_items_per_file\n",
    "                        systematic_indices = list(range(0, len(img_meta), step))[:max_items_per_file * 3 // 4]\n",
    "                        \n",
    "                        # Add random samples for extra diversity\n",
    "                        remaining_count = max_items_per_file - len(systematic_indices)\n",
    "                        if remaining_count > 0:\n",
    "                            remaining_indices = [i for i in range(len(img_meta)) if i not in systematic_indices]\n",
    "                            if remaining_indices:\n",
    "                                random_indices = random.sample(remaining_indices, \n",
    "                                                             min(remaining_count, len(remaining_indices)))\n",
    "                                all_indices = systematic_indices + random_indices\n",
    "                            else:\n",
    "                                all_indices = systematic_indices\n",
    "                        else:\n",
    "                            all_indices = systematic_indices\n",
    "                        \n",
    "                        # Sort and sample\n",
    "                        final_indices = sorted(list(set(all_indices)))[:max_items_per_file]\n",
    "                        sampled_img_meta = [img_meta[i] for i in final_indices]\n",
    "                        \n",
    "                        limited_entry['image_data'] = {'img_meta': sampled_img_meta}\n",
    "                        print(f\"  → Images: sampled {len(sampled_img_meta)} from {len(img_meta)} (DIVERSE - preserves visual diversity!)\")\n",
    "                    else:\n",
    "                        # Simple truncation (your current method)\n",
    "                        limited_entry['image_data'] = {'img_meta': img_meta[:max_items_per_file]}\n",
    "                        print(f\"  → Images: truncated to {max_items_per_file} from {len(img_meta)} (WARNING: may lose diversity)\")\n",
    "                else:\n",
    "                    limited_entry['image_data'] = {'img_meta': img_meta}\n",
    "                    print(f\"  → Images: kept all {len(img_meta)} features\")\n",
    "            else:\n",
    "                limited_entry['image_data'] = img_data\n",
    "        \n",
    "        sample_data.append(limited_entry)\n",
    "        file_count += 1\n",
    "        print(f\"✓ Added file {file_count}/{max_files}\")\n",
    "    \n",
    "    return pd.DataFrame(sample_data)\n",
    "\n",
    "# REPLACE YOUR CURRENT DATASET CREATION WITH THIS:\n",
    "print(\"=== CREATING DATASET WITH SMART SAMPLING ===\")\n",
    "combined_dataframe = create_smart_limited_dataframe(\n",
    "    max_files=3, \n",
    "    max_items_per_file=50, \n",
    "    sampling_strategy='diverse'  # This preserves image feature diversity!\n",
    ")\n",
    "\n",
    "print(f\"\\n=== SMART DATASET CREATED ===\")\n",
    "print(f\"DataFrame shape: {combined_dataframe.shape}\")\n",
    "print(f\"Memory usage: {combined_dataframe.memory_usage(deep=True).sum() / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# PROGRESSIVE SCALING STRATEGY\n",
    "# Start small for testing, then scale up for training\n",
    "\n",
    "def get_recommended_dataset_size(purpose='development'):\n",
    "    \"\"\"Get recommended dataset parameters based on purpose\"\"\"\n",
    "    configs = {\n",
    "        'development': {'files': 3, 'items': 50, 'total': 150},\n",
    "        'testing': {'files': 10, 'items': 100, 'total': 1000},\n",
    "        'training_small': {'files': 25, 'items': 200, 'total': 5000},\n",
    "        'training_medium': {'files': 50, 'items': 400, 'total': 20000},\n",
    "        'training_large': {'files': 100, 'items': 500, 'total': 50000},\n",
    "        'production': {'files': 200, 'items': 500, 'total': 100000}\n",
    "    }\n",
    "    return configs.get(purpose, configs['development'])\n",
    "\n",
    "def create_scalable_dataset(purpose='development'):\n",
    "    \"\"\"Create dataset based on purpose with memory monitoring\"\"\"\n",
    "    config = get_recommended_dataset_size(purpose)\n",
    "    \n",
    "    print(f\"\\n=== CREATING {purpose.upper()} DATASET ===\")\n",
    "    print(f\"Target: {config['files']} files × {config['items']} items = ~{config['total']} samples\")\n",
    "    \n",
    "    try:\n",
    "        df = create_smart_limited_dataframe(\n",
    "            max_files=config['files'], \n",
    "            max_items_per_file=config['items'],\n",
    "            sampling_strategy='diverse'\n",
    "        )\n",
    "        memory_in_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "        print(f\"\\n✅ SUCCESS: {purpose} dataset created\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Memory: {memory_in_mb:.2f} MB\")\n",
    "\n",
    "        if memory_in_mb > 1000:  # > 1GB\n",
    "            print(\"⚠️  WARNING: High memory usage detected\")\n",
    "        \n",
    "        return df\n",
    "    except MemoryError:\n",
    "        print(f\"❌ MEMORY ERROR: {purpose} dataset too large\")\n",
    "        print(\"Falling back to smaller dataset...\")\n",
    "        return create_scalable_dataset('development')\n",
    "\n",
    "# STEP 1: Start with development dataset for initial testing\n",
    "print(\"STEP 1: Creating development dataset for initial testing...\")\n",
    "combined_dataframe = create_scalable_dataset('development')\n",
    "\n",
    "# STEP 2: Function to upgrade dataset size when ready\n",
    "def upgrade_dataset(current_df, target_purpose='testing'):\n",
    "    \"\"\"Upgrade to larger dataset when ready\"\"\"\n",
    "    print(f\"\\n🔄 UPGRADING DATASET TO: {target_purpose}\")\n",
    "    return create_scalable_dataset(target_purpose)\n",
    "\n",
    "print(f\"\\n📊 CURRENT DATASET INFO:\")\n",
    "print(f\"Purpose: Development (for initial testing)\")\n",
    "print(f\"Samples: ~{combined_dataframe.shape[0] * 50} (estimated)\")\n",
    "print(f\"\\n🚀 TO SCALE UP FOR TRAINING:\")\n",
    "print(f\"   combined_dataframe = upgrade_dataset(combined_dataframe, 'training_small')\")\n",
    "print(f\"   # This will give you ~5,000 samples for basic training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdbb6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_dim generated, length is 2048\n",
      "Using device for clip: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:01<00:01,  1.56it/s]"
     ]
    }
   ],
   "source": [
    "# 'DATA TOKENIZATION AND EMMBEDDING USING TRANSFORMERS' and 'MultimodalBotSoftPrompt' classes would be placed here.\n",
    "\n",
    "# 0. Initialize feature_dim\n",
    "feature_dim = infer_feature_dim(combined_dataframe)\n",
    "\n",
    "# 1. Initialize the data processor\n",
    "tf_processor = TFMultimodalProcessor(combined_dataframe, feature_dim=feature_dim)\n",
    "\n",
    "# 2. Initialize the multimodal bot (this is the new, trainable model)\n",
    "embedding_dim = tf_processor.text_embedding_dim + tf_processor.feature_dim\n",
    "multimodal_bot = MultimodalBotSoftPrompt(embedding_dim=embedding_dim)  \n",
    "\n",
    "# 3. Create the generator with the trained bot\n",
    "generator = MultimodalGenerator(bot_model=multimodal_bot, processor=tf_processor)\n",
    "  \n",
    "# 4. Run the pipeline with the generator\n",
    "generator.run(combined_dataframe, limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d81fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Set up the optimizer to train the bot\n",
    "optimizer = torch.optim.AdamW(multimodal_bot.parameters(), lr=1e-5)\n",
    "num_epochs = 3\n",
    "BATCH_SIZE = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "multimodal_bot.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"--- Epoch {epoch + 1} of True Multimodal Training ---\")\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    try:\n",
    "       # The generator yields batches of data and embeddings\n",
    "        data_stream = tf_processor.process_and_embed(combined_dataframe, BATCH_SIZE=BATCH_SIZE)\n",
    "        for i, (batch_of_entries, batch_of_embeddings) in enumerate(data_stream):\n",
    "            optimizer.zero_grad()  # Clear gradients at the start\n",
    "            # Move embeddings to the correct device\n",
    "            combined_embeddings_tensor = torch.from_numpy(batch_of_embeddings).float().to(device)\n",
    "            # The bot generates refined prompts from the embeddings\n",
    "            refined_prompts = multimodal_bot(combined_embeddings_tensor)\n",
    "   \n",
    "            batch_losses = []\n",
    "            batch_idx = 0\n",
    "\n",
    "            for j, row in batch_of_entries.iterrows():\n",
    "                try:\n",
    "                    raw_text = tf_processor.extract_text(row)\n",
    "                    if not raw_text.strip():\n",
    "                        continue\n",
    "                        \n",
    "                    refined_prompt = refined_prompts[batch_idx % len(refined_prompts)]\n",
    "                    batch_idx += 1\n",
    "\n",
    "                    # Generate image and evaluate similarity using the generator's methods\n",
    "                    image = generator.generate_image(refined_prompt)\n",
    "                    similarity_score = generator.evaluate_clip_similarity(image, raw_text)\n",
    "                    \n",
    "                    # Calculate loss based on CLIP similarity (maximize similarity)\n",
    "                    loss = -torch.tensor(similarity_score, device=device, requires_grad=True)\n",
    "                    batch_losses.append(loss)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing entry {j}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if batch_losses:\n",
    "                # Average the losses and backpropagate\n",
    "                batch_loss = torch.stack(batch_losses).mean()\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(multimodal_bot.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += batch_loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                print(f\"Batch {i+1} Loss: {batch_loss.item():.4f}\")\n",
    "                \n",
    "                # Memory cleanup\n",
    "                del batch_loss, batch_losses\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in epoch {epoch + 1}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / max(batch_count, 1)\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(\"True multimodal bot training complete!\")\n",
    "    # Save the new bot model's state dictionary\n",
    "torch.save(multimodal_bot.state_dict(), \"multimodal_bot.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84835f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function pads the embedding history to a fixed length\n",
    "def pad_embeddings(embeddings_list, max_history_length=10):\n",
    "    \"\"\"Pads a list of embeddings to a fixed sequence length.\"\"\"\n",
    "    # Stack the embeddings to create a single tensor\n",
    "    stacked_embeddings = torch.stack(embeddings_list, dim=1).squeeze(0)\n",
    "    \n",
    "    # Get the current sequence length and embedding dimension\n",
    "    current_length, embedding_dim = stacked_embeddings.shape\n",
    "    \n",
    "    if current_length >= max_history_length:\n",
    "        # Truncate if the history is too long\n",
    "        return stacked_embeddings[:, -max_history_length:]\n",
    "    else:\n",
    "        # Pad with zeros if the history is too short\n",
    "        padding_tensor = torch.zeros(max_history_length - current_length, embedding_dim)\n",
    "        padded = torch.cat([stacked_embeddings, padding_tensor], dim=0)\n",
    "        return padded.unsqueeze(0) # Reshape for batch size of 1\n",
    "\n",
    "# A global variable to store the conversation history (embeddings)\n",
    "conversation_history_embeddings = []\n",
    "\n",
    "def multimodal_app(user_text, uploaded_image=None):\n",
    "    global conversation_history_embeddings\n",
    "\n",
    "    if not user_text.strip():\n",
    "        return None, \"⚠️ Please enter a valid prompt.\", \"\"\n",
    "    \n",
    "    # Add a reset command\n",
    "    if user_text.lower() == \"/reset\":\n",
    "        conversation_history_embeddings.clear()\n",
    "        return None, \"Conversation history has been reset.\", \"\"\n",
    "\n",
    "    # 1. Create a new combined embedding for the current turn\n",
    "    # This requires the `create_combined_embedding_single_turn` method in your generator\n",
    "    new_embedding = generator.create_combined_embedding(user_text, uploaded_image)\n",
    "    \n",
    "    # 2. Append the new embedding to the conversation history\n",
    "    conversation_history_embeddings.append(new_embedding)\n",
    "    \n",
    "    # 3. Pad the history to a fixed sequence length\n",
    "    padded_embeddings = pad_embeddings(conversation_history_embeddings)\n",
    "    \n",
    "    # 4. Feed the entire history to the conversational bot\n",
    "    # This requires the `refine_prompt_conversational` method in your generator\n",
    "    refined_prompt = generator.refine_prompt_multimodal(padded_embeddings)\n",
    "    \n",
    "    # 5. Generate and evaluate the image\n",
    "    generated_image = generator.generate_image(refined_prompt)\n",
    "    similarity_generated = generator.evaluate_clip_similarity(generated_image, user_text)\n",
    "    \n",
    "    # Evaluate uploaded image if available\n",
    "    similarity_uploaded = None\n",
    "    if uploaded_image is not None:\n",
    "        similarity_uploaded = generator.evaluate_clip_similarity(uploaded_image, user_text)\n",
    "\n",
    "    # Prepare the output message\n",
    "    similarity_msg = f\"🧠 Generated Image Similarity: {similarity_generated:.3f}\"\n",
    "    if similarity_uploaded is not None:\n",
    "        similarity_msg += f\"n📷 Uploaded Image Similarity: {similarity_uploaded:.3f}\"\n",
    "        \n",
    "    return generated_image, similarity_msg, refined_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db78ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'RUNNING WITH GRADIO'\n",
    "demo = gr.Interface(\n",
    "    fn=multimodal_app,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=3, label=\"Enter a prompt\"),\n",
    "        gr.Image(type=\"pil\", label=\"Upload an image (optional)\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Image(type=\"pil\", label=\"Generated Image\"),\n",
    "        gr.Textbox(label=\"Similarity Scores\"),\n",
    "        gr.Textbox(label=\"Refined Prompt\")\n",
    "    ],\n",
    "    title=\"🖼️ Multimodal Generator with CLIP Evaluation\",\n",
    "    description=\"Enter a prompt to generate an image and compare it with an uploaded image using CLIP similarity.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMAIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
